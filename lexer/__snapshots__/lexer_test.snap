
[TestBasicTokens - 1]
lexer.TokenList{
    Index:  0,
    Tokens: {
        {Token:"<", Token_content:""},
        {Token:"TEXT", Token_content:"a"},
        {Token:">", Token_content:""},
        {Token:"TEXT", Token_content:"Tag"},
        {Token:"</", Token_content:""},
        {Token:"TEXT", Token_content:"a"},
        {Token:">", Token_content:""},
    },
}
---

[TestText - 1]
lexer.TokenList{
    Index:  0,
    Tokens: {
        {Token:"TEXT", Token_content:"Just"},
        {Token:" ", Token_content:" "},
        {Token:"TEXT", Token_content:"Text"},
    },
}
---

[TestSpacedTokens - 1]
lexer.TokenList{
    Index:  0,
    Tokens: {
        {Token:"<", Token_content:""},
        {Token:"TEXT", Token_content:"a"},
        {Token:">", Token_content:""},
        {Token:"TEXT", Token_content:"Hello"},
        {Token:" ", Token_content:"    "},
        {Token:"TEXT", Token_content:"World"},
        {Token:" ", Token_content:"          "},
        {Token:"TEXT", Token_content:"Bruh"},
        {Token:"</", Token_content:""},
        {Token:"TEXT", Token_content:"a"},
        {Token:">", Token_content:""},
    },
}
---

[TestNesting - 1]
lexer.TokenList{
    Index:  0,
    Tokens: {
        {Token:"<", Token_content:""},
        {Token:"TEXT", Token_content:"hello"},
        {Token:">", Token_content:""},
        {Token:"<", Token_content:""},
        {Token:"TEXT", Token_content:"world"},
        {Token:">", Token_content:""},
        {Token:"TEXT", Token_content:"Something"},
        {Token:" ", Token_content:" "},
        {Token:"TEXT", Token_content:"And"},
        {Token:" ", Token_content:" "},
        {Token:"TEXT", Token_content:"Something"},
        {Token:"</", Token_content:""},
        {Token:"TEXT", Token_content:"world"},
        {Token:">", Token_content:""},
        {Token:"</", Token_content:""},
        {Token:"TEXT", Token_content:"hello"},
        {Token:">", Token_content:""},
    },
}
---

[TestMultipleLists - 1]
lexer.TokenList{
    Index:  0,
    Tokens: {
        {Token:" ", Token_content:"\n    "},
        {Token:"<", Token_content:""},
        {Token:"TEXT", Token_content:"hello"},
        {Token:">", Token_content:""},
        {Token:" ", Token_content:"\n      "},
        {Token:"<", Token_content:""},
        {Token:"TEXT", Token_content:"world"},
        {Token:">", Token_content:""},
        {Token:"TEXT", Token_content:"Something"},
        {Token:" ", Token_content:" "},
        {Token:"TEXT", Token_content:"here"},
        {Token:" ", Token_content:" "},
        {Token:"<", Token_content:""},
        {Token:"TEXT", Token_content:"a"},
        {Token:">", Token_content:""},
        {Token:" ", Token_content:" "},
        {Token:"</", Token_content:""},
        {Token:"TEXT", Token_content:"a"},
        {Token:">", Token_content:""},
        {Token:" ", Token_content:" "},
        {Token:"TEXT", Token_content:"??????"},
        {Token:" ", Token_content:"          \n\n\n\n\n    "},
        {Token:"TEXT", Token_content:"dsnmkadsmakkmlsdakmldsa\n\n"},
        {Token:" ", Token_content:"      "},
        {Token:"</", Token_content:""},
        {Token:"TEXT", Token_content:"world"},
        {Token:">", Token_content:""},
        {Token:"</", Token_content:""},
        {Token:"TEXT", Token_content:"hello"},
        {Token:">", Token_content:""},
        {Token:" ", Token_content:"\n    "},
    },
}
---

[TestMultipleLines - 1]
lexer.TokenList{
    Index:  0,
    Tokens: {
        {Token:" ", Token_content:"\n    "},
        {Token:"<", Token_content:""},
        {Token:"TEXT", Token_content:"hello"},
        {Token:">", Token_content:""},
        {Token:" ", Token_content:"\n      "},
        {Token:"<", Token_content:""},
        {Token:"TEXT", Token_content:"world"},
        {Token:">", Token_content:""},
        {Token:"TEXT", Token_content:"Something"},
        {Token:" ", Token_content:" "},
        {Token:"TEXT", Token_content:"here"},
        {Token:" ", Token_content:" "},
        {Token:"<", Token_content:""},
        {Token:"TEXT", Token_content:"a"},
        {Token:">", Token_content:""},
        {Token:" ", Token_content:" "},
        {Token:"</", Token_content:""},
        {Token:"TEXT", Token_content:"a"},
        {Token:">", Token_content:""},
        {Token:" ", Token_content:" "},
        {Token:"TEXT", Token_content:"??????"},
        {Token:" ", Token_content:"          \n\n\n\n\n    "},
        {Token:"TEXT", Token_content:"dsnmkadsmakkmlsdakmldsa\n\n"},
        {Token:" ", Token_content:"      "},
        {Token:"</", Token_content:""},
        {Token:"TEXT", Token_content:"world"},
        {Token:">", Token_content:""},
        {Token:"</", Token_content:""},
        {Token:"TEXT", Token_content:"hello"},
        {Token:">", Token_content:""},
        {Token:" ", Token_content:"\n    "},
    },
}
---

[TestAttributes - 1]
lexer.TokenList{
    Index:  0,
    Tokens: {
        {Token:"<", Token_content:""},
        {Token:"TEXT", Token_content:"a"},
        {Token:" ", Token_content:" "},
        {Token:"TEXT", Token_content:"hello"},
        {Token:"=", Token_content:""},
        {Token:"string_literal", Token_content:"world"},
        {Token:">", Token_content:""},
    },
}
---
